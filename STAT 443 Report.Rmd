---
title: "STAT 443 Report"
author: "Chow Sheng Liang"
date: "2025-07-26"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(forecast)
```

\newpage

# Part 1

A fundamental part of forecasting is understanding how a variable relates to its own past, this is because most real-world processes are not memoryless. For example, daily temperature tends to evolve smoothly over time, electricity demand shows strong daily and weekly cycles, and stock market volatility clusters over consecutive days.

Stationarity is a core concept that provides the theoretical framework to model such temporal relationships. Formally we define a time series as :

$$
\{X_t\}_{t \in \mathbb{Z}} \quad \text{or} \quad \{X_1, X_2, \dots, X_T\}
$$

where each $X_t$ is a random variable representing the value of a process at time $t$. The model  we are trying to approximate here would differ slightly from linear regression:

$$
\hat{f}(\{X_1, \cdots, X_t\}) \approx f(\{X_1, \cdots, X_t\}) = X_{t+1} 
$$

Stationarity is satisfied by the following 3 conditions:

1. $E(|X_t|^2) < \infty \quad \text{for all } t\in T$
2. $E(X_t) = \mu \quad \text{for all } t\in T$
3. $Cov(X_r,X_s) = Cov(X_{r+t},X_{s+t}) \quad \text{for all }\;r,s,r+t,s+t \in T$

These conditions combine to ensure that the statistical properties of a process remain **consistent over time**, allowing us to estimate future values based on past patterns without worrying about structural drift. *(Theoretically, stationarity by itself is not enough, we would also need causality that ensures a variable is **only influenced by its past**)*

Just as linear regression builds on linearity, time series analysis builds on stationarity. Under this assumption, we can fit structured models like $AR$, $MA$, or $ARMA$, which linearly relate a variable to its past values, much like how linear models relate outcomes to transformed inputs.

In practice, as you may have noticed with linear regression, we often do not observe data that adhere to the linearity/normality assumption and we typically transform $y$ or $X$. In the same spirit, we transform a time series with differencing and lag transformations:

- Backward operator: $BX_t = X_{t-1}$
- $d^{th}$ order differencing: $Y_t:= \nabla^dX_t = (1-B)^dX_t$
- Seasonal differencing: $\nabla^sX_t:= (1-B^s)X_t$


```{r, echo = FALSE, fig.width = 6, fig.height = 2, fig.cap = "Transformation of US accidental-death counts"}
data(USAccDeaths)

par(mfrow = c(1, 3),            
    mar = c(4, 4, 2, 1),
    cex.main= 0.8)        

plot(USAccDeaths,
     main = "(a) Raw counts",
     ylab = "Deaths")

plot(diff(USAccDeaths, lag = 12),
     main = "(b) Seasonal difference",
     ylab = "Deaths; season diffed")

plot(diff(diff(USAccDeaths, lag = 12)),
     main = "(c) Seasonal + first diff",
     ylab = "Deaths; season & trend diffed")
```

So in summary, just as linearity is crucial for linear regression, stationarity is a core assumption in forecasting as it restricts the function space so we can meaningfully fit and extrapolate a processâ€™ temporal relationships independent of external changes.

\newpage

# Part 2

```{r, fig.width = 8, fig.height = 4, fig.cap = "Panel (a) shows long term forecasts using an AR(1) model. Panel (b) shows long term forecasts using an ARIMA(0, 1, 0) model."}
project_data <- readRDS("projectdata.rds")
pred_num_ahead <- 100
conf_int_mult <- 1.96

# Model fitting
ar1_model <- Arima(project_data, order = c(1, 0, 0))
ar1_prediction <- predict(ar1_model, n.ahead = pred_num_ahead)

arima_model <- Arima(project_data, order = c(0, 1, 0))
arima_prediction <- predict(arima_model, n.ahead = pred_num_ahead)

# Get prediction time range
freq <- frequency(project_data)
last_time <- time(project_data)[length(project_data)]
future_time <- seq(from = last_time + 1/freq, by = 1/freq, length.out = pred_num_ahead)

# Construct vectors for CI
ar1_upper <- ar1_prediction$pred + conf_int_mult * ar1_prediction$se
ar1_lower <- ar1_prediction$pred - conf_int_mult * ar1_prediction$se

arima_upper <- arima_prediction$pred + conf_int_mult * arima_prediction$se
arima_lower <- arima_prediction$pred - conf_int_mult * arima_prediction$se

data_mean = mean(project_data)
data_var_pos = data_mean + conf_int_mult * sd(project_data)
data_var_neg = data_mean - conf_int_mult * sd(project_data)

# Layout
par(mfrow = c(1, 2), mar = c(4, 4, 2, 2))

# AR(1)
plot(project_data, xlim = c(start(project_data)[1], future_time[pred_num_ahead]), 
     ylim = c(-20, 10), main = "(a)", xlab = "Time", ylab = "Value")
lines(future_time, ar1_prediction$pred, col = "red")
lines(future_time, ar1_upper, col = "blue")
lines(future_time, ar1_lower, col = "blue")
abline(h = data_mean, col = "red", lty = 2)
abline(h = data_var_pos, col = "blue", lty = 2)
abline(h = data_var_neg, col = "blue", lty = 2)

# ARIMA(0,1,0)
plot(project_data, xlim = c(start(project_data)[1], future_time[pred_num_ahead]), 
     ylim = c(-20, 10), main = "(b)", xlab = "Time", ylab = "Value")
lines(future_time, arima_prediction$pred, col = "red")
lines(future_time, arima_upper, col = "blue")
lines(future_time, arima_lower, col = "blue")
abline(h = data_mean, col = "red", lty = 2)
abline(h = data_var_pos, col = "blue", lty = 2)
abline(h = data_var_neg, col = "blue", lty = 2)
```

\newpage 
Through the Box-Jenkins methodology, we would first examine the time-series plot, ACF, and PACF plots to visually check for any reason to be suspicious of our stationary assumption. We can also numerically test for this using the Phillips-Perron test, where the null hypothesis is of a unit root $\rightarrow$ non-stationarity; which we hope to reject. 

If we see a clear trend from the time-series plot or if the Phillips-Perron test suggests non-stationarity we may consider differencing it and re-evaluating. If we see a seasonal trend in the ACF(i.e. ACF spikes at regular periods) we may consider differencing with a lag that matches the period and re-evaluating. 

Once the process is stationary, Box-Jenkins requires us to identify the model by fitting an appropriate order and validating with a criterion, then reiterating until the one with the best metric is found. However this entire process can be much easily done with `auto.arima()` through numerical evaluation. We can optionally provide `auto.arima()` with a ceiling for its search to optimize speed, `auto.arima()` will then automate the identification process by iteratively minimizing AIC across the possible `(p,d,q)(P,D,Q)` combinations with options to include/exclude the seasonal components as well as options to exhaustively search all permutations or do a stepwise search. 

We will then visually validate this model by looking at its residual plot and residual ACF plot which we should expect to see evidence of a white noise process. If the residuals suggests otherwise, we should go back to step 1 and re-evaluate our model selection, and maybe even consider completely different models like ARCH, GARCH or simpler linear models like Holt-Winters.

\newpage
# Part 3
## Overview

The goal of this simulation study is to assess the performance of two tools commonly used in time series modeling:

1. **`auto.arima()`**:  
   This function automatically selects (S)ARIMA models by minimizing information criteria such as AIC. It searches through combinations of autoregressive (AR), differencing (I), and moving average (MA) components to find the model that minimizes prediction error.

2. **`PP.test()`**:  
   The Phillips-Perron test is a statistical procedure used to test for the presence of a unit root in an AR process (i.e., non-stationarity) against the alternative hypothesis of stationarity.

These tools will be evaluated under different simulated time series conditions to understand their reliability and performance.

## Background
We will be simulating data from an AR(1) process, which in simple terms is a process where the $t^{th}$ value depends only on the $(t-1)^{th}$ value and a i.i.d noise term. In this experiment specifically we define:
$$X_t = 0.95X_{t-1} + Z_t \qquad \text{Where }Z_t \thicksim \mathcal{N}(0, 0.5^2)$$
A stationary process is defined to have a **(a)finite second moment**, **(b)constant first moment**, and **(c)time-invariant auto-correlation**. The AR(1) process chosen for this experiment is stationary, a brief proof for AR(1) is provided below where our specific case has $\phi = 0.95$ and $\sigma = 0.5$.

\begin{minipage}[t]{0.32\textwidth}
\underline{(1) Finite Second Moment}\\

Expand $X_t$ recursively:
$$
X_t = \sum_{j=0}^\infty \phi^j Z_{t-j}
$$

Since $Z_t$ i.i.d then,
$$
\mathbb{E}[X_t^2] = \sum_{j=0}^\infty \phi^{2j} \sigma^2 = \frac{\sigma^2}{1 - \phi^2}
$$

which is finite if $|\phi| < 1$.
\end{minipage}
\hfill
\begin{minipage}[t]{0.32\textwidth}
\underline{(2) Constant Mean}\\

Take expectation:
$$
\mathbb{E}[X_t] = \mu_t= \phi \mu_{t-1} + \mathbb{E}[Z_t]
$$

Since $\mathbb{E}[Z_t] = 0$, then:
$$
\mu_t = \phi^t \mu_0
$$

The mean is constant if $\mu_0 = 0$ which is assumed in this case.
\end{minipage}
\hfill
\begin{minipage}[t]{0.32\textwidth}
\underline{(3) Time-invariant Auto-covariance}\\

Let $\gamma(h) = \text{Cov}(X_t, X_{t-h})$.
\begin{align*}
\gamma(1) &= \text{Cov}(\phi X_{t-1} + Z_t, X_{t-1}) \\
          &= \phi \, \text{Cov}(X_{t-1}, X_{t-1}) + 0 \\
          &= \phi \gamma(0)
\end{align*}

By induction:
$$
\gamma(h) = \phi^h \gamma(0)
$$

So $\gamma$ depends only on $h$. 
\end{minipage}

\vspace{1em} 
There exists a general solution for the stationarity of an AR process which you can read up in more detail in Brockwell and Davis. The general idea is that if we define a backward operator $B$ such that $BX_t =X_{t-1}$ then we can represent an AR process as a polynomial, with our AR process specifically being $(1+0.95B)X_t = Z_t$. The solution states that if there does not exist a complex unit root, then the process is stationary. 

It is also known that the closer the roots are to the unit circle, the more non-stationary the process behaves. In our case, our root would be 1.05 which is very close to the unit circle and so when sampled its stationarity is not always clear. Intuitively this also makes sense as the process will tend to drift away from the mean as it retains such a big contribution from the previous term. All this is to say that this is a process that is potentially ambiguous and serves as a good stress test for our tools.

This gives motivation towards the logic and design of our experiment. We will compare the
performance of the Phillips-Perron test across a high number of realizations of 
simulated data with a large length, and then because we know the AR(1) can 
appear non-stationary in shorter time intervals, we will test across another 1000 
realizations of simulated data to emphasize the importance of dataset size in the 
result of applying these tools (PP test and auto arima). \newpage

```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressMessages({
  suppressWarnings({
    library(forecast)
    library(tseries)
  })
})
```

```{echo=TRUE}
# Set seed for reproducibility
set.seed(443)

# Simulation function
simulate_experiment <- function(n_sim, series_length, phi = 0.95, sigma = 0.5) {
  pp_rejections <- logical(n_sim)
  arima_orders <- character(n_sim)

  for (i in 1:n_sim) {
    # Simulate AR(1) series
    e <- rnorm(series_length, mean = 0, sd = sigma)
    x <- numeric(series_length)
    x[1] <- rnorm(1, 0, sigma)
    for (t in 2:series_length) {
      x[t] <- phi * x[t - 1] + e[t]
    }

    # Phillips-Perron test
    pp_result <- suppressWarnings(pp.test(x))
    pp_rejections[i] <- pp_result$p.value < 0.05

    # auto.arima model
    fit <- auto.arima(x, ic = "aic", stationary = FALSE, seasonal = FALSE)
    ord <- arimaorder(fit)
    arima_orders[i] <- paste0("ARIMA(", ord[1], ",", ord[2], ",", ord[3], ")")
  }

  # Return results
  list(
    pp_reject_count = sum(pp_rejections),
    pp_reject_rate = mean(pp_rejections),
    arima_freq = sort(table(arima_orders), decreasing = TRUE)
  )
}

# Run experiments
results_long <- simulate_experiment(n_sim = 1000, series_length = 1000)
results_short <- simulate_experiment(n_sim = 1000, series_length = 100)

# Print results
cat("=== LONG SERIES (n = 1000) ===\n")
cat("PP Test Rejections:", results_long$pp_reject_count, "\n")
print(results_long$arima_freq)

cat("\n=== SHORT SERIES (n = 100) ===\n")
cat("PP Test Rejections:", results_short$pp_reject_count, "\n")
print(results_short$arima_freq)
```
\newpage
To assess the performance of the PP-test and the auto arima function, we evaluated
the performance of the functions in identifying stationarity and model structure
from the known AR(1) process:
$$X_t = 0.95X_{t-1} + \epsilon_t, \epsilon_t \thicksim \mathcal{N}(0, 0.5^2)$$
As we have shown the process is stationary in theory but exhibits non-stationary
properties in short intervals due to the high value of $\phi = 0.95$. We compared
results for: 
A long series with 1000 realizations of length 1000 and a 
shorter length series with 1000 realizations of length 100.

The auto.arima() function
was allowed to select from the full class of ARIMA models, including those that 
apply differencing. Although we know our simulated data is stationary, in
real-world applications the true model is unknown. As such, we set stationary = 
FALSE to reflect a more realistic setting where the algorithm must determine 
whether differencing is needed.

The PP test examines the null hypothesis that the series has a unit root 
(i.e., is non-stationary). Therefore, rejecting the null supports the conclusion 
that the series is stationary.

In the long series (length = 1000), the PP test correctly concluded the
data was stationary in all 1000 simulations.

In the shorter series (length = 100), the PP test only correctly rejected
the null hypothesis in only 119 out of 1000 simulations. In the remaining 881 cases,
the test failed to detect stationarity, indicating that short series may mislead 
formal statistical tests, such as the PP test in this case.

The auto.arima() funciton selects ARIMA models based on AIC. Ideally it should frequently
select the ARIMA(1,0,0) model which is the true model.

In the long series, the most frequent model selections were ARIMA(0,1,0): 369 times, 
ARIMA(1,0,0) 243 times, and the third most frequent ARIMA(0, 1, 1) only 40 times.

In the shorter series the ARIMA(0, 1, 0) model was selected 676 times and
the ARIMA(1,0,0) was only selected 158 times

These results show that both the Phillips-Perron test can struggle
with processes with high autocorrelation ($\phi \approx 1$), especially when sample
sizes are small. We see this as the shorter length simulations only had the PP test
correctly reject the hypothesis/ detect stationarity around 12% of the time.
These results conclude that the PP-test has low power to detect stationarity 
in short samples, particularly when autocorrelation is high, leading to many 
false conclusions of non-stationarity within time series.

The auto.arima() tends to favour differenced or unnecessarily complex models 
when the process has a high autocorrelation (phi close to 1, in this case 0.95) 
even in the samples with longer length as the most selected model was the ARIMA(0,1,0).
This shows model selection via AIC can be unstable in small samples, often choosing 
unnecessarily complex ARIMA models that do not reflect the true generating process.

Overall, the results demonstrate the importance of dataset length and the 
limitations of relying solely on automated statistical tests. When working with 
time series that exhibit strong autocorrelation, analysts must be cautious as 
short-term behavior may mimic non-stationarity and lead to incorrect model 
identification or hypothesis test results. Visual inspection, diagnostic tools,
and theoretical understanding should be used together to avoid these misunderstandings.